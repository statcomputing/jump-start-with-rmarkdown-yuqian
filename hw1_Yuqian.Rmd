---
title: "Practise Homework: Jump Start with RMarkdown"
subtitle: "Monte Carlos Approximation"
author:
  - Yuqian Liu^[<yuqian.liu@uconn.edu>; Ph.D. student at
    Department of Mechanical Engineering, University of Connecticut.]
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
bibliography: template.bib
biblio-style: datalab
keywords: Approximation, Monte Carlo, Normal Destribution, R Markdown, bookdown
# keywords set in YAML header here only go to the properties of the PDF output
# the keywords that appear in PDF output are set in latex/before_body.tex
output: 
 bookdown::pdf_document
 bookdown::html_document
abstract: |
    This is the first homework for STAT5361 Statistical Computation which is 
    designed for getting familiar with RMarkdown. In this homework, I will 
    approximate one integration by Monte Carlo method with sampling 
    from Normal distribution function $N(0,1)$. The experiments would be 
    conducted at 3 different sample sizes, with 9 different upper limits. 
    The key-step derivation and results will be presented. Ture values will
    also be calculated as reference.
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
source("utils_template.R")
source("main_Yuqian.R")

## specify the packages needed
pkgs <- c("splines2", "DT", "webshot", "leaflet")
need.packages(pkgs)

## external data can be read in by regular functions,
## such as read.table or load

## get output format in case something needs extra effort
outFormat <- knitr::opts_knit$get("rmarkdown.pandoc.to")
## "latex" or "html"

## for latex and html output
isHtml <- identical(outFormat, "html")
isLatex <- identical(outFormat, "latex")
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```


# Introduction {#sec:intro}

This homework is using the template provided by Wenjie Wang. The purpose of this 
homework is to let us get familiar with RMarkdown. 


The problem is about to approximate the value of Cumulative Distribution Function
(C. D. F.) by [Monte Carlo methods][wiki], which are computational algorithms 
that rely on repeated random sampling to obtain numerical results.The CDF of a 
continuous random variable X can be expressed as the integral of its probability 
density function. In this problem, it is Normal distribution function $N(0,1)$. 
The experiments would be conducted with 3 different sample sizes, to approximate
integration value with 9 different upper limits. Then, each case will be 
repeated 100 times.


The rest of this homework is organized as follows: Section \@ref(sec:problem)
will give problem discription. In Section \@ref(sec:math),
mathematical derivation will be illustrated. The procedure of solving this
integration will be presented step by step. The one-run results of different 
scenarios will be given in Section \@ref(sec:results). The ture value of the 
integrals will also be calculated as references. After repeating each case 
100 times, Section \@ref(sec:figure) will give box plots of the bias. Discussion
based on those plots will be presented. The code chunk of this homework's 
calculation is given in Section \@ref(sec:code). In the end, summary will 
be presented in Section \@ref(sec:summary).



# Problem Formulation{#sec:problem}


Consider approximation of the distribution function of $N(0,1)$,
\begin{align}
    \Phi (t) = \int_\infty ^t {\frac{1}{\sqrt{2\pi }}} {e^{ - {y^2}/2}}dy
    (\#eq:integral)
\end{align}
by the Monte Carlo methods:
\begin{align}
   \hat \Phi (t) = \frac{1}{n}\sum\limits_{i = 1}^n {I({X_i} \le t)}
   (\#eq:final)
\end{align}
where $X_i$'s are independent, identically distributed (i.i.d.) $N(0,1)$ variables. 


Experiment with the approximation at $n\in\{{10^2},{10^3},{10^4}\}$ at 
$t\in\{0.0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72\}$ to form a table. The 
table should include the true value for comparison. Further repeat the experiment
100 times. Draw box plots of the bias at all $t$.



# Math Derivation {#sec:math}


Consider Equation \@ref(eq:integral) in this form,
\begin{align}
    \Phi (t) = \int_{ - \infty }^\infty  {h(y)g(y)} dy
\end{align}


Based on Monte Carlo method,


**Step 1**: Define $h(y) = \left\{ {\begin{array}{*{20}{c}} 1&{y \le t}\\ 0&{y > t} \end{array}} \right.$


**Step 2**: Define $g(y) = \frac{1}{{\sqrt {2\pi } }}{e^{ - {y^2}/2}}$.
Thus, the probability distribution function is,
\begin{align}
    p(y) = \frac{{g(y)}}{{\int_{ - \infty }^\infty  {g(y)dy} }} = \frac{1}{{\sqrt {2\pi } }}{e^{ - {y^2}/2}}
\end{align}


**Step 3**: Draw $n$ i.i.d. random samples from $p(y)$.


**Step 4**: Evaluate
\begin{align}
\hat \Phi (t) = C{\rm E}[h(x)] \approx \frac{C}{n}\sum\limits_{i = 1}^n {h({x_i})}  = \frac{1}{n}\sum\limits_{i = 1}^n {I({X_i} \le t)}
\end{align}
based on samples from **Step 3**.



# Results for One-time Run  {#sec:results}

The approximations with one-time run at $n\in\{{10^2},{10^3},{10^4}\}$ and 
$t\in\{0.0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72\}$ are  shown in Table
\@ref(tab:data1). The true values of integration are also provided for comparison.


<!-- source extracted from main_yuqian.R  -->

```{r data1} 
knitr::kable(final_data_1, caption = 'Ture Values and Prediction Values', booktabs = TRUE)
```



# Results for 100-times Run {#sec:figure}

Next step is repeating each case 100 times. Box plots are generated to demonstrate results. 
Figure \@ref(fig:s100) shows predictions with 100 random samples. Predictions with 1000 
and 10000 samples are shown in Figure \@ref(fig:s1000) and Figure \@ref(fig:s10000) 
respectively. One of code chunks how to plot box plots is shown as follow.

<!-- caption of figure may be defined as follows: -->

(ref:s100) 100-run Predictions with 100 samples.

```{r s100, echo = TRUE, fig.cap = "(ref:s100)", fig.width = 8}
boxplot(Pred_100 ~ t, data = final_data_100, xlab = "t",ylab = "Predictions")
```
```{r s1000, echo = FALSE, fig.cap = "100-run Predictions with 1000 samples", fig.width = 8}
boxplot(Pred_1000 ~ t, data = final_data_100, xlab = "t",ylab = "Predictions")
```
```{r s10000,echo = FALSE, fig.cap = "100-run Predictions with 10000 samples",fig.width = 8}
boxplot(Pred_10000 ~ t, data = final_data_100, xlab = "t",ylab = "Predictions")
```

If we would like to see box plots of bias for cases with different sample sizes, Figure 
\@ref(fig:bias100), \@ref(fig:bias1000) and \@ref(fig:bias10000) are given as follow.

(ref:bias100) Bias of 100-run Predictions with 100 samples.

```{r bias100, echo = FALSE, fig.cap = "(ref:bias100)", fig.width = 8}
boxplot(bias_100 ~ t, data = final_data_bias, xlab = "t",ylab = "Prediction Bias")
```

(ref:bias1000) Bias of 100-run Predictions with 1000 samples.

```{r bias1000, echo = FALSE, fig.cap = "(ref:bias1000)", fig.width = 8}
boxplot(bias_1000 ~ t, data = final_data_bias, xlab = "t",ylab = "Prediction Bias")
```

(ref:bias10000) Bias of 100-run Predictions with 10000 samples.

```{r bias10000, echo = FALSE, fig.cap = "(ref:bias10000)", fig.width = 8}
boxplot(bias_10000 ~ t, data = final_data_bias, xlab = "t",ylab = "Prediction Bias")
```
Based on results, with increasing sample size, the predictions present smaller bias.
If the algorithm works well, we could say more accurate approximation would be obtained.


# Code Chunk {#sec:code}

The code chunk of major calculation is given as follow,
<!-- note that we actually set eval to be FALSE here so that readers are able to
reproduce this template without Python 3. -->

```{r, eval = FALSE}
t <- c(0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72)
n <- c(100,1000,10000)

# true value
ture_Phi <- pnorm(t)

# Prediction function
Pred_Phi_100 <- function(x){
  y <- rnorm(100);
  m <- sum(y<=x);
  m/100
}

Pred_Phi_1000 <- function(x){
  y <- rnorm(1000);
  m <- sum(y<=x);
  m/1000
}

Pred_Phi_10000 <- function(x){
  y <- rnorm(10000);
  m <- sum(y<=x);
  m/10000
}

## One-time experiment
# try sapply function
Pred_1_100 <- sapply(t,Pred_Phi_100)
Pred_1_1000 <- sapply(t,Pred_Phi_1000)
Pred_1_10000 <- sapply(t,Pred_Phi_10000)
final_data_1 <- data.frame(t = t, `Ture Value`= ture_Phi, `n = 100` = Pred_1_100, 
                           `n = 1000` = Pred_1_1000,
                           `n = 10000` = Pred_1_10000, stringsAsFactors = FALSE, 
                           check.names = FALSE)

## 100-time experiment
# T <- matrix(rep(t,each = 100), nrow = 100)
T <- rep(t,each = 100)
# Ture_Phi <- matrix(rep(ture_Phi,each = 100), nrow = 100)
# n = 100
Pred_100_100 <- sapply(T,Pred_Phi_100)
Pred100_100 <- array(Pred_100_100,c(100,9)) #reshape

# n = 1000
Pred_100_1000 <- sapply(T,Pred_Phi_1000)
Pred100_1000 <- array(Pred_100_1000,c(100,9)) #reshape

# n = 10000
Pred_100_10000 <- sapply(T,Pred_Phi_10000)
Pred100_10000 <- array(Pred_100_10000,c(100,9)) #reshape

final_data_100 <- data.frame(t = T, Pred_100 = Pred_100_100, Pred_1000 = Pred_100_1000, 
                             Pred_10000 = Pred_100_10000)

## get predictions' bias
# mean of predictions
mean100 <- apply(Pred100_100,2,mean)
mean1000 <- apply(Pred100_1000,2,mean)
mean10000 <- apply(Pred100_10000,2,mean)
Mean100 <- rep(mean100,each = 100)
Mean1000 <- rep(mean1000,each = 100)
Mean10000 <- rep(mean10000,each = 100)
# bias
bias100 <- Pred_100_100 - Mean100
bias1000 <- Pred_100_1000 - Mean1000
bias10000 <- Pred_100_10000 - Mean10000
final_data_bias <- data.frame(t = T, bias_100 = bias100, bias_1000 = bias1000, 
                              bias_10000 = bias10000)
```

The chunk option `eval = FALSE` is set, in order to present the code without evaluation.



# Summary {#sec:summary}

I approximated the integration accurately by Monte Carlo Methods. I used the 
template to report what I have done for this practise.  I tried some functions 
and syntax of writing a $.Rmd$ document. Overall, through this homework, I am 
getting familiar with R and RMarkdown. 



# Acknowledgment {-}

Thanks to Wenjie Wang's template, which isavailable at GitHub repository
*dslab-templates*: <https://github.com/statds/dslab-templates>.
And thanks to all the other authors and contributors.


# Reference {-}


[wiki]: https://en.wikipedia.org/wiki/Monte_Carlo_method
